{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning Practices\n",
    "\n",
    "In this lab, we will apply learned concepts from Day 1-2 lectures to perform data cleaning on a given Airbnb data set. \n",
    "\n",
    "This dataset (raw.csv) contains 30k+ records on hotels in the top-10 tourist destinations and major US metropolitan areas sraped from Airbnb.com. \n",
    "Each data record has 40 attributes including the number of bedrooms, price, location, etc. \n",
    "The attribute \"pop2016\" means population of the zipcode location (area) in year 2016.\n",
    "Demographic and economic attributes were scraped from city-data.com. \n",
    "\n",
    "Updated: short description for attributes:\n",
    "\n",
    "## House specific features, collected from Airbnb.com:\n",
    "Bathrooms: The number of bathrooms in the listing\n",
    "Bedrooms: The number of bedrooms\n",
    "Beds: The number of bed(s)\n",
    "LocationName: Location of the house\n",
    "NumGuests: Maximum number of guests can hold\n",
    "NumReviews: number of reviews received\n",
    "Price: daily price in local currency\n",
    "# Rating: Y/N - whether the rating of each house is 5 or not\n",
    "latitude: location information latitude\n",
    "longitude: location information longitude\n",
    "zipcode: zipcode of the house\n",
    "\n",
    "## demographic and economic attributes based on zipcode, collected from city-data.com (means the same zipcode should share the same value for each of the following attributes)\n",
    "pop2016: popularity of the area reported in 2016\n",
    "pop2010: popularity of the area reported in 2010\n",
    "pop2000: popularity of the area reported in 2000\n",
    "cost_living_index: a U.S standarded index for cost living measurement\n",
    "land_area: space of land\n",
    "water_area: space of water area\n",
    "pop_density: density of population \n",
    "number of males: within the area population\n",
    "number of females: within the area population\n",
    "prop taxes paid 2016: Median real estate property taxes paid for housing units in 2016\n",
    "median taxes: median of taxes paid by house owners in the area\n",
    "median house value: median of house value in the area\n",
    "median household income: median of income of house owners in the area\n",
    "median monthly onwer costs (with mortgage): median monthly cost of house owner including mortgage\n",
    "median monthly onwer costs (no mortgage): median monthly cost of house owner without considering mortgage\n",
    "median gross rent: the monthly rent agreed or contracted for plus the estimated monthly cost of utilities and fuels.\n",
    "median asking price for vacant for-sale home/condo: median asking price for for-sale home in the area\n",
    "unemployment: umemployment ratio of the area\n",
    "\n",
    "## aggregated features for Abnb by zipcode \n",
    "Number of Homes\tCount of Abnb:\tnumber of Abnb houses in this area\n",
    "Density of Abnb (%): ratio of Abnb houses in this area\n",
    "Average Abnb Price (by zipcode): aggregated by zipcode\n",
    "Average NumReviews (by zipcode): aggregated by zipcode\t\n",
    "Average Rating (by zipcode): aggregated by zipcode\n",
    "Average Number of Bathrooms (by zipcode): aggregated by zipcode\n",
    "Average Number of Bedrooms (by zipcode): aggregated by zipcode\n",
    "Average Number of Beds (by zipcode): aggregated by zipcode\n",
    "Average Number of Guests (by zipcode): aggregated by zipcode\n",
    "\n",
    "\n",
    "\n",
    "The prediction label is Rating of house.\n",
    "\n",
    "## Submission: submit via onq. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import needed libraries. E.g., pandas, missingno, and sklearn\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import missingno\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option('display.max_rows', None)  \n",
    "pd.set_option('display.max_columns', None) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 1: Read dataset and perform basic data exploration. Specially, you should write code to explore the types of data provided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('raw.csv')\n",
    "print(f\"There are {df.shape[0]} rows and {df.shape[1]} features\")\n",
    "df.sample(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 1, put your code here to perform data type and data scale check\n",
    "print(df.info())\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Rating'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#distribution graph for each features that hasn't null values.\n",
    "for column in df.columns:\n",
    "    if df[column].dtype != object and df[column].isna().values.any() == False:\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        sns.displot(df[column], kind=\"kde\")\n",
    "        plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 2: Data quality check, does duplicate entries exit in this table? Do they have consistent values? Briefely explain your methodology and your findings within this markdown cell, and write corresponding code in the next code cell.\n",
    "\n",
    "1- Does duplicate entries exit in this table? yes, there are duplicated entires in this dataframe.<br>\n",
    "2- Do they have consistent values? yes, there are consistent duplicated values in this dataframe.\n",
    "\n",
    "Methodology:\n",
    "1. Check if there are any duplicated rows in this table.\n",
    "2. Check if the number of duplicated rows in this table.\n",
    "3. Drop any duplicated rows and keep the first occurances.\n",
    "\n",
    "Finidings: 12.83% of the data is consistent duplicated values (duplicated entries through all features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 2\n",
    "if df.duplicated().any()== True:\n",
    "    print(\"There are duplicated values in this dataset\")\n",
    "else:\n",
    "    print(\"There aren't duplicated values in this dataset\")\n",
    "\n",
    "\n",
    "print(f\"There are {df.duplicated().sum()} consistent duplicated values in this dataset.\")\n",
    "df[df.duplicated()].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop duplicated values and keep the first occurance\n",
    "df = df.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 3: Data quality check, write code and answer:\n",
    "3.1 does missing value exit in the table? \n",
    "3.2 Where are the missing data? \n",
    "3.3 How much data is missing?\n",
    "3.4 Are there any variables often missing together？\n",
    "\n",
    "You can use missingno library to generate plots to support your claim. \n",
    "Summarize your findings for task 3 in this markdown cell and write corresponding code in the next code cell.\n",
    "\n",
    "1. does missing value exit in the table? yes there are missing values in some attributes in this dataset, There exist no record missing all variables. \n",
    "2. How much data is missing? you can see the following table to see how much data is missing in each attribute.\n",
    "3. Are there any variables often missing together？yes, there are based on heatmap graph.\n",
    "* 1. **(pop2010, pop2016, cost_living_index)** highly correlated in missing values.\n",
    "* 2. **(median taxes (with mortage), median taxes (no mortage), median house value, median monthly owner costs (with mortage), monthly owner costs (no mortage))** highly correlated in missing values.\n",
    "* 3. **(median gross rent, median asking price for vacant for-sale home/condo, unemployment (%), median asking price for vacant for-sale home/condo)** medium correlated values. \n",
    "* 4. **('Beds', 'LocationName', 'NumGuests', 'NumReviews', 'Price')**\n",
    "\n",
    "These columns are highly correlated in missing values. highly correlated in missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 3\n",
    "\n",
    "if df.isna().values.any() == True:\n",
    "    #this condidtion will be evaluated.\n",
    "    print(\"There are missing values in this dataframe\")\n",
    "else:\n",
    "    print(\"There aren't missing values in this dataframe\")\n",
    "total_miss = df.isnull().sum()\n",
    "percent_miss = (total_miss/df.isnull().count()*100)\n",
    "\n",
    "# sort attributes by missing value ratio\n",
    "missing_data = pd.DataFrame({'Total missing':total_miss,'% missing':percent_miss})\n",
    "missing_data.sort_values(by='Total missing',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missingno.bar(df)\n",
    "missingno.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: What are the potential mechnisms of the missing values? Briefely explain your methodology and your findings (within this markdown cell), and write corresponding code in the next code cell.\n",
    "\n",
    "methodology:\n",
    "\n",
    "1. plot the histogram of each attribute that has missing value. \n",
    "2. plot the missing value matrix for the dataset.\n",
    "3. determine if there is a specific pattern for missing values in each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 4\n",
    "for column in df.columns:\n",
    "    if df[column].dtype != object and df[column].isna().values.any() == True:\n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        #sns.displot(df[column], kind=\"hist\")\n",
    "        df.hist(column)\n",
    "        plt.close(fig)\n",
    "        \n",
    "missingno.matrix(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "findings for each attribute have missing values (why data are missing for each attribute and what's the pattern?)\n",
    "\n",
    "1. **Bathrooms, Beds, LocationName, NumGuests, NumReviews, pop2010, 2016, cost_living_index, Number of homes , Density of Abnb (%)** they are could be **MAR(missing at random)** mechanism, since they are missed values continuously in a specific preiod of time randomly and they return back to be normal and didn't loss values again and no assumsions here were found to say they that follow **MNAR** mechanism.\n",
    "2. the rest of attributes that has missing values they follow **MCAR (missing completely at Random)** mechanism since they are haven't known pattern in missing values so we can't determine why they are losing these values at these times like (**Bedrooms** attribute).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Handling missing values, Briefely explain your methodology below (within this markdown cell), and write corresponding code in the next code cell.\n",
    "\n",
    "Methdology (fill missing value in each attribute with simple imputation technique):\n",
    "\n",
    "1. fill missing object attribute with the most frequented value.\n",
    "2. fill missing integer attributes with the rounded mean value in each feature.\n",
    "2. fill missing float attributes the with mean value in each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 5\n",
    "df['LocationName'] = df['LocationName'].fillna(df['LocationName'].mode()[0])\n",
    "for column in df.columns:\n",
    "    if df[column].dtype != object and df[column].isna().values.any() == True:\n",
    "        #check if the value is integer or it has 0 float point like 1.0000 fillna here is to ignore null values and it won't effect in the feature.\n",
    "        if df[column].dtype == int or df[column].fillna(-9999).apply(float.is_integer).all() == True:\n",
    "            df[column] = df[column].fillna(round(df[column].mean()))\n",
    "        elif df[column].dtype == float or df[column].fillna(-9999).apply(float.is_integer).all() == False:\n",
    "            df[column] = df[column].fillna(df[column].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Impact on classification performance. Consider one of the above handling method you proposed for this dataset and perform classification tast to investigate if your handling method can improve classificaiton performance. \n",
    "\n",
    "Train-test split: you can do one split of train and test where 70% of the data for training and the remaining 30% for testing. \n",
    "Classifier: you can pick any two tranditional binary classifier (e.g., from sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO for Task 6\n",
    "LE  = LabelEncoder()\n",
    "#Let's convert Rating and LocationName name attribute to numerical values\n",
    "df['Rating'] = LE.fit_transform(df['Rating'])\n",
    "df['LocationName'] = LE.fit_transform(df['LocationName'])\n",
    "#Let's convert Rating and LocationName from integer values to categorical values.\n",
    "df['Rating'] = df['Rating'].astype('category')\n",
    "\n",
    "y = df['Rating']\n",
    "df = df.drop('Rating',axis=1)\n",
    "\n",
    "columns = list(df.columns)\n",
    "\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(df)\n",
    "df = pd.DataFrame(x_scaled,columns=columns)\n",
    "df['LocationName'] = df['LocationName'].astype('category')\n",
    "#split the data to train and test.\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, stratify=y, train_size= 0.7 , shuffle=True)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the classifier\n",
    "DT_classifier = DecisionTreeClassifier()\n",
    "#train the classifier\n",
    "DT_classifier.fit(X_train,y_train)\n",
    "#test the classifier\n",
    "y_predicted = DT_classifier.predict(X_test)\n",
    "#evaluate the classifier\n",
    "print(f\"The accuracy score is {round(accuracy_score(y_test, y_predicted)*100,2)}%.\")\n",
    "print(classification_report(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Impact:\n",
    "\n",
    "we could improve the performance if we try different preprocessing techniques like handling outliers, oversampling the dataset or try another null values handling like deletion technique by droping some feature that has a lot of null values or trying other classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the classifier\n",
    "LG_classifier = LogisticRegression(max_iter=10000)\n",
    "#train the classifier\n",
    "LG_classifier.fit(X_train,y_train)\n",
    "#test the classifier\n",
    "y_predicted = LG_classifier.predict(X_test)\n",
    "#evaluate the classifier\n",
    "print(f\"The accuracy score is {round(accuracy_score(y_test, y_predicted)*100,2)}%.\")\n",
    "print(classification_report(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#building the classifier\n",
    "GB_classifier = RandomForestClassifier()\n",
    "#train the classifier\n",
    "GB_classifier.fit(X_train,y_train)\n",
    "#test the classifier\n",
    "y_predicted = GB_classifier.predict(X_test)\n",
    "#evaluate the classifier\n",
    "print(f\"The accuracy score is {round(accuracy_score(y_test, y_predicted)*100,2)}%.\")\n",
    "print(classification_report(y_test,y_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Report your findings through the above experiments (in this markdown cell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. the data consists of 33145 rows and 40 features, 39 features as input and one feature as output.\n",
    "2. The number of **No** values in the output features represent approximately the half number of **Yes** values. \n",
    "3. **12.83%** of the data is consistent duplicated values (duplicated entries through all features).\n",
    "4. some features has missing values and some of them follow MCRA (missing completely at random) mechanism and the others are following the (MAR) mechanism.\n",
    "5. some of features often missing together？\n",
    "6. most of the features has normal distribution.\n",
    "7. Between Logistic regression and decision tree classifier and Random forest classifier the last classifier was the best.\n",
    "8. we could improve the performance if we try different preprocessing techniques like handling outliers or trying other classifiers."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7dc369991a124e20fb8d6bc9586253148146ab3e7709482e52796b6ee563b03a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 ('python310')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
